{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n",
      "num res blocks: 18\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/default/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-1-c75431a6ee83>:32: fractional_avg_pool (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` and `deterministic` args are deprecated.  Use fractional_avg_pool_v2.\n",
      "26\n",
      "21\n",
      "16\n",
      "res now 16.0\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/default/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "13\n",
      "10\n",
      "8\n",
      "res now 8.0\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/default/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Learning rate:  0.001\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (50, 32, 32, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (50, 32, 32, 16)     448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (50, 32, 32, 16)     64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (50, 32, 32, 16)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (50, 32, 32, 16)     2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (50, 32, 32, 16)     64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (50, 32, 32, 16)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (50, 32, 32, 16)     2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (50, 32, 32, 16)     64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (50, 32, 32, 16)     0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (50, 32, 32, 16)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (50, 32, 32, 16)     2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (50, 32, 32, 16)     64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (50, 32, 32, 16)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (50, 32, 32, 16)     2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (50, 32, 32, 16)     64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (50, 32, 32, 16)     0           activation_3[0][0]               \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (50, 32, 32, 16)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (50, 32, 32, 16)     2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (50, 32, 32, 16)     64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (50, 32, 32, 16)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (50, 32, 32, 16)     2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (50, 32, 32, 16)     64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (50, 32, 32, 16)     0           activation_5[0][0]               \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (50, 32, 32, 16)     0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (50, 32, 32, 16)     2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (50, 32, 32, 16)     64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (50, 32, 32, 16)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (50, 32, 32, 16)     2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (50, 32, 32, 16)     64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (50, 32, 32, 16)     0           activation_7[0][0]               \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (50, 32, 32, 16)     0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (50, 32, 32, 16)     2320        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (50, 32, 32, 16)     64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (50, 32, 32, 16)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (50, 32, 32, 16)     2320        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (50, 32, 32, 16)     64          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (50, 32, 32, 16)     0           activation_9[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (50, 32, 32, 16)     0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (50, 32, 32, 16)     2320        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (50, 32, 32, 16)     64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (50, 32, 32, 16)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (50, 32, 32, 16)     2320        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (50, 32, 32, 16)     64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (50, 32, 32, 16)     0           activation_11[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (50, 32, 32, 16)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (50, 32, 32, 16)     2320        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (50, 32, 32, 16)     64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (50, 32, 32, 16)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (50, 32, 32, 16)     2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (50, 32, 32, 16)     64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (50, 32, 32, 16)     0           activation_13[0][0]              \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (50, 32, 32, 16)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (50, 32, 32, 16)     2320        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (50, 32, 32, 16)     64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (50, 32, 32, 16)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (50, 32, 32, 16)     2320        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (50, 32, 32, 16)     64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (50, 32, 32, 16)     0           activation_15[0][0]              \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (50, 32, 32, 16)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (50, 32, 32, 16)     2320        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (50, 32, 32, 16)     64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (50, 32, 32, 16)     0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (50, 32, 32, 16)     2320        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (50, 32, 32, 16)     64          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (50, 32, 32, 16)     0           activation_17[0][0]              \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (50, 32, 32, 16)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (50, 32, 32, 16)     2320        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (50, 32, 32, 16)     64          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (50, 32, 32, 16)     0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (50, 32, 32, 16)     2320        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (50, 32, 32, 16)     64          conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (50, 32, 32, 16)     0           activation_19[0][0]              \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (50, 32, 32, 16)     0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (50, 32, 32, 16)     2320        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (50, 32, 32, 16)     64          conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (50, 32, 32, 16)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (50, 32, 32, 16)     2320        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (50, 32, 32, 16)     64          conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (50, 32, 32, 16)     0           activation_21[0][0]              \n",
      "                                                                 batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (50, 32, 32, 16)     0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (50, 32, 32, 16)     2320        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (50, 32, 32, 16)     64          conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (50, 32, 32, 16)     0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (50, 32, 32, 16)     2320        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (50, 32, 32, 16)     64          conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (50, 32, 32, 16)     0           activation_23[0][0]              \n",
      "                                                                 batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (50, 32, 32, 16)     0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (50, 32, 32, 16)     2320        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (50, 32, 32, 16)     64          conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (50, 32, 32, 16)     0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (50, 32, 32, 16)     2320        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (50, 32, 32, 16)     64          conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (50, 32, 32, 16)     0           activation_25[0][0]              \n",
      "                                                                 batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (50, 32, 32, 16)     0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (50, 32, 32, 16)     2320        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (50, 32, 32, 16)     64          conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (50, 32, 32, 16)     0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (50, 32, 32, 16)     2320        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (50, 32, 32, 16)     64          conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (50, 32, 32, 16)     0           activation_27[0][0]              \n",
      "                                                                 batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (50, 32, 32, 16)     0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (50, 32, 32, 16)     2320        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (50, 32, 32, 16)     64          conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (50, 32, 32, 16)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (50, 32, 32, 16)     2320        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (50, 32, 32, 16)     64          conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (50, 32, 32, 16)     0           activation_29[0][0]              \n",
      "                                                                 batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (50, 32, 32, 16)     0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (50, 32, 32, 16)     2320        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (50, 32, 32, 16)     64          conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (50, 32, 32, 16)     0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (50, 32, 32, 16)     2320        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (50, 32, 32, 16)     64          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (50, 32, 32, 16)     0           activation_31[0][0]              \n",
      "                                                                 batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (50, 32, 32, 16)     0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (50, 32, 32, 16)     2320        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (50, 32, 32, 16)     64          conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (50, 32, 32, 16)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (50, 32, 32, 16)     2320        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (50, 32, 32, 16)     64          conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (50, 32, 32, 16)     0           activation_33[0][0]              \n",
      "                                                                 batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (50, 32, 32, 16)     0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (50, 32, 32, 16)     2320        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (50, 32, 32, 16)     64          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (50, 32, 32, 16)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (50, 32, 32, 16)     2320        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (50, 32, 32, 16)     64          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (50, 32, 32, 16)     0           activation_35[0][0]              \n",
      "                                                                 batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (50, 32, 32, 16)     0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fractional_pooling2d_1 (Fractio (50, 26, 26, 16)     0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (50, 26, 26, 32)     4640        fractional_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (50, 26, 26, 32)     128         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (50, 26, 26, 32)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (50, 26, 26, 32)     9248        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (50, 26, 26, 32)     544         fractional_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (50, 26, 26, 32)     128         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (50, 26, 26, 32)     0           conv2d_40[0][0]                  \n",
      "                                                                 batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (50, 26, 26, 32)     0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (50, 26, 26, 32)     9248        activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (50, 26, 26, 32)     128         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (50, 26, 26, 32)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (50, 26, 26, 32)     9248        activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (50, 26, 26, 32)     128         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (50, 26, 26, 32)     0           activation_39[0][0]              \n",
      "                                                                 batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (50, 26, 26, 32)     0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (50, 26, 26, 32)     9248        activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (50, 26, 26, 32)     128         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (50, 26, 26, 32)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (50, 26, 26, 32)     9248        activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (50, 26, 26, 32)     128         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (50, 26, 26, 32)     0           activation_41[0][0]              \n",
      "                                                                 batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (50, 26, 26, 32)     0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (50, 26, 26, 32)     9248        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (50, 26, 26, 32)     128         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (50, 26, 26, 32)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (50, 26, 26, 32)     9248        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (50, 26, 26, 32)     128         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (50, 26, 26, 32)     0           activation_43[0][0]              \n",
      "                                                                 batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (50, 26, 26, 32)     0           add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (50, 26, 26, 32)     9248        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (50, 26, 26, 32)     128         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (50, 26, 26, 32)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (50, 26, 26, 32)     9248        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (50, 26, 26, 32)     128         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (50, 26, 26, 32)     0           activation_45[0][0]              \n",
      "                                                                 batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (50, 26, 26, 32)     0           add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (50, 26, 26, 32)     9248        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (50, 26, 26, 32)     128         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (50, 26, 26, 32)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (50, 26, 26, 32)     9248        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (50, 26, 26, 32)     128         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (50, 26, 26, 32)     0           activation_47[0][0]              \n",
      "                                                                 batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (50, 26, 26, 32)     0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fractional_pooling2d_2 (Fractio (50, 21, 21, 32)     0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (50, 21, 21, 32)     9248        fractional_pooling2d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (50, 21, 21, 32)     128         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (50, 21, 21, 32)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (50, 21, 21, 32)     9248        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (50, 21, 21, 32)     128         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (50, 21, 21, 32)     0           fractional_pooling2d_2[0][0]     \n",
      "                                                                 batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (50, 21, 21, 32)     0           add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (50, 21, 21, 32)     9248        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (50, 21, 21, 32)     128         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (50, 21, 21, 32)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (50, 21, 21, 32)     9248        activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (50, 21, 21, 32)     128         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (50, 21, 21, 32)     0           activation_51[0][0]              \n",
      "                                                                 batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (50, 21, 21, 32)     0           add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (50, 21, 21, 32)     9248        activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (50, 21, 21, 32)     128         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (50, 21, 21, 32)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (50, 21, 21, 32)     9248        activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (50, 21, 21, 32)     128         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (50, 21, 21, 32)     0           activation_53[0][0]              \n",
      "                                                                 batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (50, 21, 21, 32)     0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (50, 21, 21, 32)     9248        activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (50, 21, 21, 32)     128         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (50, 21, 21, 32)     0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (50, 21, 21, 32)     9248        activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (50, 21, 21, 32)     128         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (50, 21, 21, 32)     0           activation_55[0][0]              \n",
      "                                                                 batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (50, 21, 21, 32)     0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (50, 21, 21, 32)     9248        activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (50, 21, 21, 32)     128         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (50, 21, 21, 32)     0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (50, 21, 21, 32)     9248        activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (50, 21, 21, 32)     128         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (50, 21, 21, 32)     0           activation_57[0][0]              \n",
      "                                                                 batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (50, 21, 21, 32)     0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (50, 21, 21, 32)     9248        activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (50, 21, 21, 32)     128         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (50, 21, 21, 32)     0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (50, 21, 21, 32)     9248        activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (50, 21, 21, 32)     128         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (50, 16, 16, 16)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (50, 21, 21, 32)     0           activation_59[0][0]              \n",
      "                                                                 batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (50, 16, 16, 32)     544         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (50, 21, 21, 32)     0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (50, 16, 16, 32)     128         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "fractional_pooling2d_3 (Fractio (50, 16, 16, 32)     0           activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (50, 16, 16, 32)     0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (50, 16, 16, 32)     0           fractional_pooling2d_3[0][0]     \n",
      "                                                                 activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (50, 16, 16, 32)     9248        add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (50, 16, 16, 32)     128         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (50, 16, 16, 32)     0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (50, 16, 16, 32)     9248        activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (50, 16, 16, 32)     128         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (50, 16, 16, 32)     0           add_31[0][0]                     \n",
      "                                                                 batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (50, 16, 16, 32)     0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (50, 16, 16, 32)     9248        activation_64[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (50, 16, 16, 32)     128         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (50, 16, 16, 32)     0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (50, 16, 16, 32)     9248        activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (50, 16, 16, 32)     128         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (50, 16, 16, 32)     0           activation_64[0][0]              \n",
      "                                                                 batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (50, 16, 16, 32)     0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (50, 16, 16, 32)     9248        activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (50, 16, 16, 32)     128         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (50, 16, 16, 32)     0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (50, 16, 16, 32)     9248        activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (50, 16, 16, 32)     128         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (50, 16, 16, 32)     0           activation_66[0][0]              \n",
      "                                                                 batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (50, 16, 16, 32)     0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (50, 16, 16, 32)     9248        activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (50, 16, 16, 32)     128         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (50, 16, 16, 32)     0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (50, 16, 16, 32)     9248        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (50, 16, 16, 32)     128         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (50, 16, 16, 32)     0           activation_68[0][0]              \n",
      "                                                                 batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (50, 16, 16, 32)     0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (50, 16, 16, 32)     9248        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (50, 16, 16, 32)     128         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (50, 16, 16, 32)     0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (50, 16, 16, 32)     9248        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (50, 16, 16, 32)     128         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (50, 16, 16, 32)     0           activation_70[0][0]              \n",
      "                                                                 batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (50, 16, 16, 32)     0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (50, 16, 16, 32)     9248        activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (50, 16, 16, 32)     128         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (50, 16, 16, 32)     0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (50, 16, 16, 32)     9248        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (50, 16, 16, 32)     128         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (50, 16, 16, 32)     0           activation_72[0][0]              \n",
      "                                                                 batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (50, 16, 16, 32)     0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fractional_pooling2d_4 (Fractio (50, 13, 13, 32)     0           activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (50, 13, 13, 64)     18496       fractional_pooling2d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (50, 13, 13, 64)     256         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (50, 13, 13, 64)     0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (50, 13, 13, 64)     36928       activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (50, 13, 13, 64)     2112        fractional_pooling2d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (50, 13, 13, 64)     256         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (50, 13, 13, 64)     0           conv2d_78[0][0]                  \n",
      "                                                                 batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (50, 13, 13, 64)     0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (50, 13, 13, 64)     36928       activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (50, 13, 13, 64)     256         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (50, 13, 13, 64)     0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (50, 13, 13, 64)     36928       activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (50, 13, 13, 64)     256         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (50, 13, 13, 64)     0           activation_76[0][0]              \n",
      "                                                                 batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (50, 13, 13, 64)     0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (50, 13, 13, 64)     36928       activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (50, 13, 13, 64)     256         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (50, 13, 13, 64)     0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (50, 13, 13, 64)     36928       activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (50, 13, 13, 64)     256         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (50, 13, 13, 64)     0           activation_78[0][0]              \n",
      "                                                                 batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (50, 13, 13, 64)     0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (50, 13, 13, 64)     36928       activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (50, 13, 13, 64)     256         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (50, 13, 13, 64)     0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (50, 13, 13, 64)     36928       activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (50, 13, 13, 64)     256         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (50, 13, 13, 64)     0           activation_80[0][0]              \n",
      "                                                                 batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (50, 13, 13, 64)     0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (50, 13, 13, 64)     36928       activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (50, 13, 13, 64)     256         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (50, 13, 13, 64)     0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (50, 13, 13, 64)     36928       activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (50, 13, 13, 64)     256         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (50, 13, 13, 64)     0           activation_82[0][0]              \n",
      "                                                                 batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (50, 13, 13, 64)     0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (50, 13, 13, 64)     36928       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (50, 13, 13, 64)     256         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (50, 13, 13, 64)     0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (50, 13, 13, 64)     36928       activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (50, 13, 13, 64)     256         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (50, 13, 13, 64)     0           activation_84[0][0]              \n",
      "                                                                 batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (50, 13, 13, 64)     0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fractional_pooling2d_5 (Fractio (50, 10, 10, 64)     0           activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (50, 10, 10, 64)     36928       fractional_pooling2d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (50, 10, 10, 64)     256         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (50, 10, 10, 64)     0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (50, 10, 10, 64)     36928       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (50, 10, 10, 64)     256         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (50, 10, 10, 64)     0           fractional_pooling2d_5[0][0]     \n",
      "                                                                 batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (50, 10, 10, 64)     0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (50, 10, 10, 64)     36928       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (50, 10, 10, 64)     256         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (50, 10, 10, 64)     0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (50, 10, 10, 64)     36928       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (50, 10, 10, 64)     256         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (50, 10, 10, 64)     0           activation_88[0][0]              \n",
      "                                                                 batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (50, 10, 10, 64)     0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (50, 10, 10, 64)     36928       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (50, 10, 10, 64)     256         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (50, 10, 10, 64)     0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (50, 10, 10, 64)     36928       activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (50, 10, 10, 64)     256         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (50, 10, 10, 64)     0           activation_90[0][0]              \n",
      "                                                                 batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (50, 10, 10, 64)     0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (50, 10, 10, 64)     36928       activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (50, 10, 10, 64)     256         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (50, 10, 10, 64)     0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (50, 10, 10, 64)     36928       activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (50, 10, 10, 64)     256         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (50, 10, 10, 64)     0           activation_92[0][0]              \n",
      "                                                                 batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (50, 10, 10, 64)     0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (50, 10, 10, 64)     36928       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (50, 10, 10, 64)     256         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (50, 10, 10, 64)     0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (50, 10, 10, 64)     36928       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (50, 10, 10, 64)     256         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (50, 10, 10, 64)     0           activation_94[0][0]              \n",
      "                                                                 batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (50, 10, 10, 64)     0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (50, 10, 10, 64)     36928       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (50, 10, 10, 64)     256         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (50, 10, 10, 64)     0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (50, 10, 10, 64)     36928       activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (50, 10, 10, 64)     256         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (50, 8, 8, 16)       0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (50, 10, 10, 64)     0           activation_96[0][0]              \n",
      "                                                                 batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (50, 8, 8, 64)       1088        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (50, 10, 10, 64)     0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (50, 8, 8, 64)       256         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fractional_pooling2d_6 (Fractio (50, 8, 8, 64)       0           activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (50, 8, 8, 64)       0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (50, 8, 8, 64)       0           fractional_pooling2d_6[0][0]     \n",
      "                                                                 activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (50, 8, 8, 64)       36928       add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (50, 8, 8, 64)       256         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (50, 8, 8, 64)       0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (50, 8, 8, 64)       36928       activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (50, 8, 8, 64)       256         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (50, 8, 8, 64)       0           add_50[0][0]                     \n",
      "                                                                 batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (50, 8, 8, 64)       0           add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (50, 8, 8, 64)       36928       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (50, 8, 8, 64)       256         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (50, 8, 8, 64)       0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (50, 8, 8, 64)       36928       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (50, 8, 8, 64)       256         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (50, 8, 8, 64)       0           activation_101[0][0]             \n",
      "                                                                 batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (50, 8, 8, 64)       0           add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (50, 8, 8, 64)       36928       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (50, 8, 8, 64)       256         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (50, 8, 8, 64)       0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (50, 8, 8, 64)       36928       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (50, 8, 8, 64)       256         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (50, 8, 8, 64)       0           activation_103[0][0]             \n",
      "                                                                 batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (50, 8, 8, 64)       0           add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (50, 8, 8, 64)       36928       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (50, 8, 8, 64)       256         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (50, 8, 8, 64)       0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (50, 8, 8, 64)       36928       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (50, 8, 8, 64)       256         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (50, 8, 8, 64)       0           activation_105[0][0]             \n",
      "                                                                 batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (50, 8, 8, 64)       0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (50, 8, 8, 64)       36928       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (50, 8, 8, 64)       256         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (50, 8, 8, 64)       0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (50, 8, 8, 64)       36928       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (50, 8, 8, 64)       256         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (50, 8, 8, 64)       0           activation_107[0][0]             \n",
      "                                                                 batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (50, 8, 8, 64)       0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (50, 8, 8, 64)       36928       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (50, 8, 8, 64)       256         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (50, 8, 8, 64)       0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (50, 8, 8, 64)       36928       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (50, 8, 8, 64)       256         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (50, 8, 8, 64)       0           activation_109[0][0]             \n",
      "                                                                 batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (50, 8, 8, 64)       0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (50, 1, 1, 64)       0           activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (50, 64)             0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (50, 10)             650         flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,744,778\n",
      "Trainable params: 1,736,490\n",
      "Non-trainable params: 8,288\n",
      "__________________________________________________________________________________________________\n",
      "ResNet110v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/default/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 173s 173ms/step - loss: 2.3683 - accuracy: 0.4606 - val_loss: 2.1565 - val_accuracy: 0.5048\n",
      "Epoch 2/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 1.6668 - accuracy: 0.6438 - val_loss: 2.6544 - val_accuracy: 0.4170\n",
      "Epoch 3/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 1.3405 - accuracy: 0.7117 - val_loss: 1.3212 - val_accuracy: 0.6997\n",
      "Epoch 4/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 1.1310 - accuracy: 0.7477 - val_loss: 1.5094 - val_accuracy: 0.6457\n",
      "Epoch 5/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.9798 - accuracy: 0.7736 - val_loss: 1.2959 - val_accuracy: 0.6908\n",
      "Epoch 6/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 151s 151ms/step - loss: 0.8779 - accuracy: 0.7942 - val_loss: 3.0006 - val_accuracy: 0.6666\n",
      "Epoch 7/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 145s 145ms/step - loss: 0.8021 - accuracy: 0.8119 - val_loss: 1.0483 - val_accuracy: 0.7340\n",
      "Epoch 8/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.7562 - accuracy: 0.8251 - val_loss: 1.1808 - val_accuracy: 0.7018\n",
      "Epoch 9/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.7076 - accuracy: 0.8366 - val_loss: 1.0617 - val_accuracy: 0.7390\n",
      "Epoch 10/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.6599 - accuracy: 0.8517 - val_loss: 70.1530 - val_accuracy: 0.2653\n",
      "Epoch 11/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.6482 - accuracy: 0.8609 - val_loss: 1.0915 - val_accuracy: 0.7419\n",
      "Epoch 12/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.5952 - accuracy: 0.8756 - val_loss: 15.3963 - val_accuracy: 0.4397\n",
      "Epoch 13/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.5670 - accuracy: 0.8859 - val_loss: 1.0560 - val_accuracy: 0.7478\n",
      "Epoch 14/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.5300 - accuracy: 0.8955 - val_loss: 1.0605 - val_accuracy: 0.7548\n",
      "Epoch 15/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 150s 150ms/step - loss: 0.5061 - accuracy: 0.9042 - val_loss: 1.1049 - val_accuracy: 0.7602\n",
      "Epoch 16/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.4864 - accuracy: 0.9121 - val_loss: 1.3381 - val_accuracy: 0.7136\n",
      "Epoch 17/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 150s 150ms/step - loss: 0.4925 - accuracy: 0.9156 - val_loss: 1.1652 - val_accuracy: 0.7461\n",
      "Epoch 18/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 154s 154ms/step - loss: 0.4732 - accuracy: 0.9235 - val_loss: 2.4324 - val_accuracy: 0.6574\n",
      "Epoch 19/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 155s 155ms/step - loss: 0.4673 - accuracy: 0.9276 - val_loss: 1.1490 - val_accuracy: 0.7622\n",
      "Epoch 20/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 152s 152ms/step - loss: 0.4499 - accuracy: 0.9321 - val_loss: 1.2215 - val_accuracy: 0.7516\n",
      "Epoch 21/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 155s 155ms/step - loss: 0.4440 - accuracy: 0.9338 - val_loss: 332.1254 - val_accuracy: 0.2455\n",
      "Epoch 22/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 152s 152ms/step - loss: 0.4442 - accuracy: 0.9367 - val_loss: 61.2406 - val_accuracy: 0.5984\n",
      "Epoch 23/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 151s 151ms/step - loss: 0.4332 - accuracy: 0.9407 - val_loss: 9.9671 - val_accuracy: 0.5909\n",
      "Epoch 24/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 151s 151ms/step - loss: 0.4355 - accuracy: 0.9423 - val_loss: 1.2179 - val_accuracy: 0.7609\n",
      "Epoch 25/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 150s 150ms/step - loss: 0.4343 - accuracy: 0.9399 - val_loss: 2.1760 - val_accuracy: 0.6848\n",
      "Epoch 26/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 156s 156ms/step - loss: 0.4346 - accuracy: 0.9432 - val_loss: 1.6197 - val_accuracy: 0.7087\n",
      "Epoch 27/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 156s 156ms/step - loss: 0.4174 - accuracy: 0.9483 - val_loss: 1.2664 - val_accuracy: 0.7533\n",
      "Epoch 28/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 152s 152ms/step - loss: 0.4228 - accuracy: 0.9464 - val_loss: 1.2671 - val_accuracy: 0.7532\n",
      "Epoch 29/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 152s 152ms/step - loss: 0.4154 - accuracy: 0.9487 - val_loss: 1.4697 - val_accuracy: 0.7398\n",
      "Epoch 30/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.4201 - accuracy: 0.9507 - val_loss: 1.3411 - val_accuracy: 0.7526\n",
      "Epoch 31/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.4098 - accuracy: 0.9525 - val_loss: 1.3445 - val_accuracy: 0.7444\n",
      "Epoch 32/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.4249 - accuracy: 0.9501 - val_loss: 1.2751 - val_accuracy: 0.7685\n",
      "Epoch 33/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.4105 - accuracy: 0.9534 - val_loss: 1.3755 - val_accuracy: 0.7513\n",
      "Epoch 34/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.4109 - accuracy: 0.9516 - val_loss: 1.3958 - val_accuracy: 0.7409\n",
      "Epoch 35/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.4072 - accuracy: 0.9535 - val_loss: 1.1540 - val_accuracy: 0.7735\n",
      "Epoch 36/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.3972 - accuracy: 0.9558 - val_loss: 1.1715 - val_accuracy: 0.7644\n",
      "Epoch 37/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.4087 - accuracy: 0.9537 - val_loss: 1.2194 - val_accuracy: 0.7650\n",
      "Epoch 38/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.4043 - accuracy: 0.9541 - val_loss: 1.4996 - val_accuracy: 0.7374\n",
      "Epoch 39/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.4115 - accuracy: 0.9561 - val_loss: 1.5630 - val_accuracy: 0.7260\n",
      "Epoch 40/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.4015 - accuracy: 0.9563 - val_loss: 1.4050 - val_accuracy: 0.7446\n",
      "Epoch 41/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.3984 - accuracy: 0.9564 - val_loss: 1.3825 - val_accuracy: 0.7438\n",
      "Epoch 42/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.4006 - accuracy: 0.9561 - val_loss: 8.8184 - val_accuracy: 0.4443\n",
      "Epoch 43/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.4018 - accuracy: 0.9583 - val_loss: 1.2607 - val_accuracy: 0.7712\n",
      "Epoch 44/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.3979 - accuracy: 0.9580 - val_loss: 1.1897 - val_accuracy: 0.7779\n",
      "Epoch 45/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 148s 148ms/step - loss: 0.3917 - accuracy: 0.9578 - val_loss: 10.5544 - val_accuracy: 0.4923\n",
      "Epoch 46/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 149s 149ms/step - loss: 0.4102 - accuracy: 0.9574 - val_loss: 1.4381 - val_accuracy: 0.7457\n",
      "Epoch 47/200\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.3955 - accuracy: 0.9578 - val_loss: 1.5124 - val_accuracy: 0.7383\n",
      "Epoch 48/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 145s 145ms/step - loss: 0.3883 - accuracy: 0.9617 - val_loss: 1.3856 - val_accuracy: 0.7485\n",
      "Epoch 49/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.3912 - accuracy: 0.9593 - val_loss: 1.3395 - val_accuracy: 0.7554\n",
      "Epoch 50/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.3798 - accuracy: 0.9612 - val_loss: 1.3192 - val_accuracy: 0.7655\n",
      "Epoch 51/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3864 - accuracy: 0.9580 - val_loss: 1.3187 - val_accuracy: 0.7637\n",
      "Epoch 52/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3843 - accuracy: 0.9604 - val_loss: 1.4585 - val_accuracy: 0.7467\n",
      "Epoch 53/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3772 - accuracy: 0.9615 - val_loss: 1.7119 - val_accuracy: 0.7014\n",
      "Epoch 54/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3805 - accuracy: 0.9593 - val_loss: 14.0073 - val_accuracy: 0.1465\n",
      "Epoch 55/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3959 - accuracy: 0.9577 - val_loss: 1.3565 - val_accuracy: 0.7626\n",
      "Epoch 56/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3880 - accuracy: 0.9619 - val_loss: 1.3566 - val_accuracy: 0.7584\n",
      "Epoch 57/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3881 - accuracy: 0.9604 - val_loss: 1.4306 - val_accuracy: 0.7474\n",
      "Epoch 58/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3803 - accuracy: 0.9620 - val_loss: 1.4425 - val_accuracy: 0.7462\n",
      "Epoch 59/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3753 - accuracy: 0.9627 - val_loss: 1.6824 - val_accuracy: 0.7144\n",
      "Epoch 60/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3851 - accuracy: 0.9599 - val_loss: 1.3056 - val_accuracy: 0.7616\n",
      "Epoch 61/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3859 - accuracy: 0.9594 - val_loss: 1.4321 - val_accuracy: 0.7401\n",
      "Epoch 62/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3771 - accuracy: 0.9641 - val_loss: 1.2573 - val_accuracy: 0.7721\n",
      "Epoch 63/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3813 - accuracy: 0.9599 - val_loss: 67.4165 - val_accuracy: 0.1855\n",
      "Epoch 64/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3822 - accuracy: 0.9603 - val_loss: 1.4019 - val_accuracy: 0.7497\n",
      "Epoch 65/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3770 - accuracy: 0.9626 - val_loss: 1.4538 - val_accuracy: 0.7507\n",
      "Epoch 66/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3808 - accuracy: 0.9618 - val_loss: 1.5843 - val_accuracy: 0.7152\n",
      "Epoch 67/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3774 - accuracy: 0.9637 - val_loss: 1.2286 - val_accuracy: 0.7740\n",
      "Epoch 68/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3771 - accuracy: 0.9634 - val_loss: 1.3688 - val_accuracy: 0.7651\n",
      "Epoch 69/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3822 - accuracy: 0.9612 - val_loss: 1.8750 - val_accuracy: 0.7088\n",
      "Epoch 70/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3787 - accuracy: 0.9627 - val_loss: 1.5006 - val_accuracy: 0.7517\n",
      "Epoch 71/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3750 - accuracy: 0.9622 - val_loss: 1.5493 - val_accuracy: 0.7329\n",
      "Epoch 72/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3780 - accuracy: 0.9616 - val_loss: 1.4400 - val_accuracy: 0.7475\n",
      "Epoch 73/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3752 - accuracy: 0.9643 - val_loss: 1.3569 - val_accuracy: 0.7610\n",
      "Epoch 74/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3732 - accuracy: 0.9646 - val_loss: 1.3188 - val_accuracy: 0.7636\n",
      "Epoch 75/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3742 - accuracy: 0.9635 - val_loss: 1.3472 - val_accuracy: 0.7551\n",
      "Epoch 76/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3721 - accuracy: 0.9637 - val_loss: 1.4220 - val_accuracy: 0.7564\n",
      "Epoch 77/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3685 - accuracy: 0.9651 - val_loss: 1.5121 - val_accuracy: 0.7510\n",
      "Epoch 78/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3715 - accuracy: 0.9637 - val_loss: 1.3470 - val_accuracy: 0.7516\n",
      "Epoch 79/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3763 - accuracy: 0.9620 - val_loss: 1.3025 - val_accuracy: 0.7734\n",
      "Epoch 80/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.3744 - accuracy: 0.9635 - val_loss: 1.6274 - val_accuracy: 0.7352\n",
      "Epoch 81/200\n",
      "Learning rate:  0.001\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.3721 - accuracy: 0.9642 - val_loss: 1.2776 - val_accuracy: 0.7741\n",
      "Epoch 82/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.3087 - accuracy: 0.9872 - val_loss: 1.0828 - val_accuracy: 0.8074\n",
      "Epoch 83/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.2784 - accuracy: 0.9973 - val_loss: 1.0980 - val_accuracy: 0.8079\n",
      "Epoch 84/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.2665 - accuracy: 0.9989 - val_loss: 1.1200 - val_accuracy: 0.8105\n",
      "Epoch 85/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.2558 - accuracy: 0.9991 - val_loss: 1.1521 - val_accuracy: 0.8101\n",
      "Epoch 86/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.2447 - accuracy: 0.9995 - val_loss: 1.1670 - val_accuracy: 0.8079\n",
      "Epoch 87/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.2321 - accuracy: 0.9997 - val_loss: 1.1774 - val_accuracy: 0.8091\n",
      "Epoch 88/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.2202 - accuracy: 0.9997 - val_loss: 1.2080 - val_accuracy: 0.8097\n",
      "Epoch 89/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.2077 - accuracy: 0.9996 - val_loss: 1.2344 - val_accuracy: 0.8088\n",
      "Epoch 90/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1971 - accuracy: 0.9995 - val_loss: 1.2482 - val_accuracy: 0.8047\n",
      "Epoch 91/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1874 - accuracy: 0.9995 - val_loss: 1.2857 - val_accuracy: 0.8058\n",
      "Epoch 92/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1785 - accuracy: 0.9997 - val_loss: 1.2638 - val_accuracy: 0.8069\n",
      "Epoch 93/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1718 - accuracy: 0.9993 - val_loss: 1.2692 - val_accuracy: 0.8086\n",
      "Epoch 94/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1650 - accuracy: 0.9994 - val_loss: 1.2918 - val_accuracy: 0.8058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1588 - accuracy: 0.9997 - val_loss: 1.2746 - val_accuracy: 0.8073\n",
      "Epoch 96/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1529 - accuracy: 0.9997 - val_loss: 1.2988 - val_accuracy: 0.8091\n",
      "Epoch 97/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1478 - accuracy: 0.9995 - val_loss: 1.3465 - val_accuracy: 0.8055\n",
      "Epoch 98/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1440 - accuracy: 0.9992 - val_loss: 1.3189 - val_accuracy: 0.8042\n",
      "Epoch 99/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1397 - accuracy: 0.9993 - val_loss: 1.3502 - val_accuracy: 0.8015\n",
      "Epoch 100/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1355 - accuracy: 0.9995 - val_loss: 1.3177 - val_accuracy: 0.8063\n",
      "Epoch 101/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1322 - accuracy: 0.9994 - val_loss: 1.3281 - val_accuracy: 0.8040\n",
      "Epoch 102/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1292 - accuracy: 0.9994 - val_loss: 1.3227 - val_accuracy: 0.8047\n",
      "Epoch 103/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1269 - accuracy: 0.9991 - val_loss: 1.3399 - val_accuracy: 0.8038\n",
      "Epoch 104/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1230 - accuracy: 0.9995 - val_loss: 1.3369 - val_accuracy: 0.8040\n",
      "Epoch 105/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1201 - accuracy: 0.9995 - val_loss: 1.3498 - val_accuracy: 0.8056\n",
      "Epoch 106/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1181 - accuracy: 0.9991 - val_loss: 1.4089 - val_accuracy: 0.7997\n",
      "Epoch 107/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1147 - accuracy: 0.9995 - val_loss: 1.3639 - val_accuracy: 0.8004\n",
      "Epoch 108/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1128 - accuracy: 0.9993 - val_loss: 1.3557 - val_accuracy: 0.8039\n",
      "Epoch 109/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1114 - accuracy: 0.9990 - val_loss: 1.3517 - val_accuracy: 0.8063\n",
      "Epoch 110/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1093 - accuracy: 0.9992 - val_loss: 1.3842 - val_accuracy: 0.7985\n",
      "Epoch 111/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1065 - accuracy: 0.9995 - val_loss: 1.3850 - val_accuracy: 0.8015\n",
      "Epoch 112/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1051 - accuracy: 0.9992 - val_loss: 1.3465 - val_accuracy: 0.8042\n",
      "Epoch 113/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1039 - accuracy: 0.9989 - val_loss: 1.3543 - val_accuracy: 0.8004\n",
      "Epoch 114/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.1010 - accuracy: 0.9994 - val_loss: 1.3647 - val_accuracy: 0.8039\n",
      "Epoch 115/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0994 - accuracy: 0.9994 - val_loss: 1.3877 - val_accuracy: 0.8013\n",
      "Epoch 116/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0990 - accuracy: 0.9989 - val_loss: 1.4509 - val_accuracy: 0.7971\n",
      "Epoch 117/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0980 - accuracy: 0.9988 - val_loss: 1.3468 - val_accuracy: 0.8033\n",
      "Epoch 118/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0956 - accuracy: 0.9991 - val_loss: 1.4132 - val_accuracy: 0.7964\n",
      "Epoch 119/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0943 - accuracy: 0.9991 - val_loss: 1.3864 - val_accuracy: 0.8010\n",
      "Epoch 120/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0923 - accuracy: 0.9993 - val_loss: 1.3661 - val_accuracy: 0.8056\n",
      "Epoch 121/200\n",
      "Learning rate:  0.0001\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0928 - accuracy: 0.9987 - val_loss: 1.4038 - val_accuracy: 0.8008\n",
      "Epoch 122/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0906 - accuracy: 0.9992 - val_loss: 1.3235 - val_accuracy: 0.8078\n",
      "Epoch 123/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0895 - accuracy: 0.9994 - val_loss: 1.3153 - val_accuracy: 0.8084\n",
      "Epoch 124/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0886 - accuracy: 0.9997 - val_loss: 1.3155 - val_accuracy: 0.8072\n",
      "Epoch 125/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0881 - accuracy: 0.9999 - val_loss: 1.3074 - val_accuracy: 0.8099\n",
      "Epoch 126/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0877 - accuracy: 0.9998 - val_loss: 1.3199 - val_accuracy: 0.8094\n",
      "Epoch 127/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0871 - accuracy: 0.9999 - val_loss: 1.3220 - val_accuracy: 0.8085\n",
      "Epoch 128/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0867 - accuracy: 0.9999 - val_loss: 1.3223 - val_accuracy: 0.8087\n",
      "Epoch 129/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0865 - accuracy: 0.9998 - val_loss: 1.3200 - val_accuracy: 0.8086\n",
      "Epoch 130/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0857 - accuracy: 1.0000 - val_loss: 1.3243 - val_accuracy: 0.8083\n",
      "Epoch 131/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0854 - accuracy: 0.9999 - val_loss: 1.3300 - val_accuracy: 0.8083\n",
      "Epoch 132/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0848 - accuracy: 1.0000 - val_loss: 1.3311 - val_accuracy: 0.8080\n",
      "Epoch 133/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0843 - accuracy: 1.0000 - val_loss: 1.3382 - val_accuracy: 0.8086\n",
      "Epoch 134/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0837 - accuracy: 1.0000 - val_loss: 1.3411 - val_accuracy: 0.8079\n",
      "Epoch 135/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0832 - accuracy: 0.9999 - val_loss: 1.3481 - val_accuracy: 0.8085\n",
      "Epoch 136/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0825 - accuracy: 1.0000 - val_loss: 1.3505 - val_accuracy: 0.8072\n",
      "Epoch 137/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0820 - accuracy: 1.0000 - val_loss: 1.3457 - val_accuracy: 0.8088\n",
      "Epoch 138/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0814 - accuracy: 1.0000 - val_loss: 1.3386 - val_accuracy: 0.8092\n",
      "Epoch 139/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0808 - accuracy: 0.9999 - val_loss: 1.3443 - val_accuracy: 0.8091\n",
      "Epoch 140/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0802 - accuracy: 0.9999 - val_loss: 1.3483 - val_accuracy: 0.8093\n",
      "Epoch 141/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0796 - accuracy: 1.0000 - val_loss: 1.3474 - val_accuracy: 0.8081\n",
      "Epoch 142/200\n",
      "Learning rate:  1e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0791 - accuracy: 1.0000 - val_loss: 1.3682 - val_accuracy: 0.8082\n",
      "Epoch 143/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0784 - accuracy: 1.0000 - val_loss: 1.3546 - val_accuracy: 0.8087\n",
      "Epoch 144/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0778 - accuracy: 1.0000 - val_loss: 1.3610 - val_accuracy: 0.8095\n",
      "Epoch 145/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0771 - accuracy: 1.0000 - val_loss: 1.3590 - val_accuracy: 0.8093\n",
      "Epoch 146/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0762 - accuracy: 1.0000 - val_loss: 1.3605 - val_accuracy: 0.8094\n",
      "Epoch 147/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0754 - accuracy: 1.0000 - val_loss: 1.3563 - val_accuracy: 0.8109\n",
      "Epoch 148/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0747 - accuracy: 1.0000 - val_loss: 1.3653 - val_accuracy: 0.8104\n",
      "Epoch 149/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0740 - accuracy: 1.0000 - val_loss: 1.3668 - val_accuracy: 0.8108\n",
      "Epoch 150/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 145s 145ms/step - loss: 0.0732 - accuracy: 1.0000 - val_loss: 1.3692 - val_accuracy: 0.8119\n",
      "Epoch 151/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 145s 145ms/step - loss: 0.0724 - accuracy: 1.0000 - val_loss: 1.3704 - val_accuracy: 0.8098\n",
      "Epoch 152/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0716 - accuracy: 1.0000 - val_loss: 1.3750 - val_accuracy: 0.8110\n",
      "Epoch 153/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0708 - accuracy: 1.0000 - val_loss: 1.3845 - val_accuracy: 0.8093\n",
      "Epoch 154/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0699 - accuracy: 1.0000 - val_loss: 1.3849 - val_accuracy: 0.8086\n",
      "Epoch 155/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0690 - accuracy: 1.0000 - val_loss: 1.3820 - val_accuracy: 0.8101\n",
      "Epoch 156/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0680 - accuracy: 1.0000 - val_loss: 1.3874 - val_accuracy: 0.8093\n",
      "Epoch 157/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0670 - accuracy: 1.0000 - val_loss: 1.3915 - val_accuracy: 0.8074\n",
      "Epoch 158/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0661 - accuracy: 1.0000 - val_loss: 1.3940 - val_accuracy: 0.8069\n",
      "Epoch 159/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0654 - accuracy: 1.0000 - val_loss: 1.3914 - val_accuracy: 0.8080\n",
      "Epoch 160/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0648 - accuracy: 1.0000 - val_loss: 1.4007 - val_accuracy: 0.8081\n",
      "Epoch 161/200\n",
      "Learning rate:  1e-05\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0642 - accuracy: 1.0000 - val_loss: 1.3873 - val_accuracy: 0.8093\n",
      "Epoch 162/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0638 - accuracy: 1.0000 - val_loss: 1.3888 - val_accuracy: 0.8093\n",
      "Epoch 163/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0637 - accuracy: 1.0000 - val_loss: 1.3927 - val_accuracy: 0.8098\n",
      "Epoch 164/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0637 - accuracy: 1.0000 - val_loss: 1.3926 - val_accuracy: 0.8091\n",
      "Epoch 165/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0635 - accuracy: 1.0000 - val_loss: 1.3945 - val_accuracy: 0.8091\n",
      "Epoch 166/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0635 - accuracy: 1.0000 - val_loss: 1.3973 - val_accuracy: 0.8079\n",
      "Epoch 167/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 1.3886 - val_accuracy: 0.8083\n",
      "Epoch 168/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0633 - accuracy: 1.0000 - val_loss: 1.3917 - val_accuracy: 0.8094\n",
      "Epoch 169/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0633 - accuracy: 1.0000 - val_loss: 1.3907 - val_accuracy: 0.8095\n",
      "Epoch 170/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0632 - accuracy: 1.0000 - val_loss: 1.3958 - val_accuracy: 0.8088\n",
      "Epoch 171/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0631 - accuracy: 1.0000 - val_loss: 1.3950 - val_accuracy: 0.8087\n",
      "Epoch 172/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0630 - accuracy: 1.0000 - val_loss: 1.3929 - val_accuracy: 0.8092\n",
      "Epoch 173/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0629 - accuracy: 1.0000 - val_loss: 1.3938 - val_accuracy: 0.8084\n",
      "Epoch 174/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0628 - accuracy: 1.0000 - val_loss: 1.3919 - val_accuracy: 0.8091\n",
      "Epoch 175/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0628 - accuracy: 1.0000 - val_loss: 1.3928 - val_accuracy: 0.8089\n",
      "Epoch 176/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0627 - accuracy: 1.0000 - val_loss: 1.3910 - val_accuracy: 0.8084\n",
      "Epoch 177/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 1.3905 - val_accuracy: 0.8082\n",
      "Epoch 178/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0625 - accuracy: 1.0000 - val_loss: 1.3923 - val_accuracy: 0.8081\n",
      "Epoch 179/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0624 - accuracy: 1.0000 - val_loss: 1.3912 - val_accuracy: 0.8081\n",
      "Epoch 180/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0624 - accuracy: 1.0000 - val_loss: 1.3910 - val_accuracy: 0.8091\n",
      "Epoch 181/200\n",
      "Learning rate:  1e-06\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0623 - accuracy: 1.0000 - val_loss: 1.3945 - val_accuracy: 0.8098\n",
      "Epoch 182/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0622 - accuracy: 1.0000 - val_loss: 1.3939 - val_accuracy: 0.8086\n",
      "Epoch 183/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0622 - accuracy: 1.0000 - val_loss: 1.3970 - val_accuracy: 0.8081\n",
      "Epoch 184/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0621 - accuracy: 1.0000 - val_loss: 1.3921 - val_accuracy: 0.8091\n",
      "Epoch 185/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 145s 145ms/step - loss: 0.0621 - accuracy: 1.0000 - val_loss: 1.3945 - val_accuracy: 0.8087\n",
      "Epoch 186/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0621 - accuracy: 1.0000 - val_loss: 1.3990 - val_accuracy: 0.8088\n",
      "Epoch 187/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0620 - accuracy: 1.0000 - val_loss: 1.3989 - val_accuracy: 0.8086\n",
      "Epoch 188/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0620 - accuracy: 1.0000 - val_loss: 1.3922 - val_accuracy: 0.8093\n",
      "Epoch 189/200\n",
      "Learning rate:  5e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0619 - accuracy: 1.0000 - val_loss: 1.3937 - val_accuracy: 0.8090\n",
      "Epoch 190/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0619 - accuracy: 1.0000 - val_loss: 1.3923 - val_accuracy: 0.8086\n",
      "Epoch 191/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0619 - accuracy: 1.0000 - val_loss: 1.3946 - val_accuracy: 0.8085\n",
      "Epoch 192/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0618 - accuracy: 1.0000 - val_loss: 1.3927 - val_accuracy: 0.8092\n",
      "Epoch 193/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0618 - accuracy: 1.0000 - val_loss: 1.3940 - val_accuracy: 0.8088\n",
      "Epoch 194/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0617 - accuracy: 1.0000 - val_loss: 1.3966 - val_accuracy: 0.8095\n",
      "Epoch 195/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0617 - accuracy: 1.0000 - val_loss: 1.3934 - val_accuracy: 0.8098\n",
      "Epoch 196/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0616 - accuracy: 1.0000 - val_loss: 1.3971 - val_accuracy: 0.8094\n",
      "Epoch 197/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0616 - accuracy: 1.0000 - val_loss: 1.3973 - val_accuracy: 0.8096\n",
      "Epoch 198/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0616 - accuracy: 1.0000 - val_loss: 1.3972 - val_accuracy: 0.8086\n",
      "Epoch 199/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 147s 147ms/step - loss: 0.0615 - accuracy: 1.0000 - val_loss: 1.3989 - val_accuracy: 0.8087\n",
      "Epoch 200/200\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 146s 146ms/step - loss: 0.0615 - accuracy: 1.0000 - val_loss: 1.3950 - val_accuracy: 0.8088\n",
      "10000/10000 [==============================] - 9s 942us/step\n",
      "Test loss: 0.9142847796678543\n",
      "Test accuracy: 0.516805112361908\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "\n",
    "class FractionalPooling2D(Layer):\n",
    "\tdef __init__(self, pool_ratio = None, pseudo_random = False, overlap = False, name ='FractionPooling2D', **kwargs):\n",
    "\t\tself.pool_ratio = pool_ratio\n",
    "\t\tself.input_spec = [InputSpec(ndim=4)]\n",
    "\t\tself.pseudo_random = pseudo_random\n",
    "\t\tself.overlap = overlap\n",
    "\t\tself.name = name\n",
    "\t\tsuper(FractionalPooling2D, self).__init__(**kwargs)\n",
    "\t\t\n",
    "\tdef call(self, input):\n",
    "\t\t[batch_tensor,row_pooling,col_pooling] = tf.nn.fractional_avg_pool(input, pooling_ratio = self.pool_ratio, pseudo_random = self.pseudo_random, overlapping = self.overlap, seed = 0)\n",
    "\t\treturn(batch_tensor)\n",
    "\t\t\n",
    "\tdef compute_output_shape(self, input_shape):\n",
    "\t\n",
    "\t\tif(K.common.image_dim_ordering() == 'channels_last' or K.common.image_dim_ordering() == 'tf'):\n",
    "\t\t\tif(input_shape[0] != None):\n",
    "\t\t\t\tbatch_size = int(input_shape[0]/self.pool_ratio[0])\n",
    "\t\t\telse:\n",
    "\t\t\t\tbatch_size = input_shape[0]\n",
    "\t\t\twidth = int(input_shape[1]/self.pool_ratio[1])\n",
    "\t\t\theight = int(input_shape[2]/self.pool_ratio[2])\n",
    "\t\t\tchannels = int(input_shape[3]/self.pool_ratio[3])\n",
    "\t\t\treturn(batch_size, width, height, channels)\n",
    "\t\t\t\n",
    "\t\telif(K.image_dim_ordering() == 'channels_first' or K.image_dim_ordering() == 'th'):\n",
    "\t\t\tif(input_shape[0] != None):\n",
    "\t\t\t\tbatch_size = int(input_shape[0]/self.pool_ratio[0])\n",
    "\t\t\telse:\n",
    "\t\t\t\tbatch_size = input_shape[0]\n",
    "\t\t\tchannels = int(input_shape[1]/self.pool_ratio[1])\n",
    "\t\t\twidth = int(input_shape[2]/self.pool_ratio[2])\n",
    "\t\t\theight = int(input_shape[3]/self.pool_ratio[3])\n",
    "\t\t\treturn(batch_size, channels, width, height)\n",
    "\t\t\n",
    "\tdef get_config(self):\n",
    "\t\tconfig = {'pooling_ratio': self.pool_ratio, 'pseudo_random': self.pseudo_random, 'overlap': self.overlap, 'name':self.name}\n",
    "\t\tbase_config = super(FractionalPooling2D, self).get_config()\n",
    "\t\treturn dict(list(base_config.items()) + list(config.items()))\n",
    "\t\t\n",
    "\tdef build(self, input_shape):\n",
    "\t\tself.input_spec = [InputSpec(shape=input_shape)]\n",
    "\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 50  # orig paper trained all networks with batch_size=128\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "\n",
    "# Subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = True\n",
    "\n",
    "# Model parameter\n",
    "# ----------------------------------------------------------------------------\n",
    "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
    "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
    "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
    "# ----------------------------------------------------------------------------\n",
    "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
    "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
    "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
    "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
    "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
    "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
    "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
    "# ---------------------------------------------------------------------------\n",
    "n = 18\n",
    "\n",
    "# Model version\n",
    "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "version = 1\n",
    "\n",
    "# Computed depth from supplied model parameter n\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2\n",
    "\n",
    "# Model name, depth and version\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "    print('num res blocks:', num_res_blocks)\n",
    "    act_size = 32.0\n",
    "\n",
    "    inputs = Input(shape=input_shape, batch_shape=(batch_size, )+input_shape)\n",
    "    #inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    top = x\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                ratio = 1.23\n",
    "                x = FractionalPooling2D((1.0,ratio, ratio,1.0), overlap=True)(x)\n",
    "                act_size = int(act_size / ratio)\n",
    "                print(act_size)\n",
    "\n",
    "\n",
    "            if stack > 0 and res_block == 6:  # first layer but not first stack\n",
    "                ratio = 1.238\n",
    "                x = FractionalPooling2D((1.0,ratio,ratio,1.0), overlap=True)(x) \n",
    "                act_size = int(act_size / ratio)\n",
    "                print(act_size)\n",
    "\n",
    "\n",
    "            if stack > 0 and res_block == 12:  # first layer but not first stack\n",
    "                ratio = 1.25\n",
    "                x = FractionalPooling2D((1.0,ratio, ratio,1.0), overlap=True)(x)\n",
    "                act_size = int(act_size / ratio)\n",
    "                print(act_size)\n",
    "\n",
    "                print('res now', 32/(2*stack))\n",
    "                y = MaxPooling2D(2*stack)(top)\n",
    "                y = Convolution2D(num_filters, (1,1), kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(y)\n",
    "                y = BatchNormalization()(y)\n",
    "                y = Activation('relu')(y)\n",
    "                x = Add()([x, y])\n",
    "\n",
    "\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            \n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model_type)\n",
    "plot_model(model, 'model.png', show_shapes=True)\n",
    "\n",
    "# Prepare model model saving directory.\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [ \n",
    "    lr_reducer, \n",
    "    lr_scheduler]\n",
    "\n",
    "# Run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator()\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    # datagen.fit(x_train)\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=epochs, verbose=1, workers=4,\n",
    "                        callbacks=callbacks)\n",
    "    # model.fit(x_train, y_train, batch_size=batch_size, callbacks=callbacks, epochs=epochs, verbose=1, workers=4)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 10s 964us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.39825473934412, 0.808899998664856]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/202\n",
      "Learning rate:  5e-07\n",
      "1000/1000 [==============================] - 167s 167ms/step - loss: 0.0614 - accuracy: 1.0000 - val_loss: 1.3983 - val_accuracy: 0.8089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa7480023d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        epochs=202, verbose=1, workers=4,\n",
    "                        callbacks=callbacks, initial_epoch=201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2651097774505615, 0.5168690085411072]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(datagen.flow(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('default': conda)",
   "language": "python",
   "name": "python37564bitdefaultconda51b6f06d5fcc47e7898ed76be30e10a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
