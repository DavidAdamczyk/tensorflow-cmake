{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from user_ops import ft_pool\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import math\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', '..', 'keras_frac'))\n",
    "from fractional_maxpooling import FractionalPooling2D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pool_net:\n",
    "    def __init__(self, opt, batch_input_shape, classes, k_size=(3,3), filters=2, blocks=2):\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.InputLayer(batch_input_shape=batch_input_shape))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same'))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same'))\n",
    "        self.model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "        for i in range(2, blocks+1):\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same'))\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same'))\n",
    "            self.model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "        self.model.add(keras.layers.Flatten())\n",
    "        self.model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "        self.model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    def train(self, batch_size, epochs, datagen=None, train_data=None, callbacks=None):\n",
    "        if datagen is None and train_data is None:\n",
    "            print('neither data or generator was passed')\n",
    "        elif datagen is None and train_data is not None:\n",
    "            print('training on array data, network type:', type(self).__name__)\n",
    "            self.history = self.model.fit(x = train_data[0], y = train_data[1], validation_data=train_data[2:], batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
    "        else:\n",
    "            print('training on datagen data, network type:', type(self).__name__)\n",
    "            self.history = self.model.fit_generator(datagen, validation_data=train_data[2:], batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
    "        self.weights = self.model.get_weights()\n",
    "    \n",
    "    def restart_session(self):\n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def load_weights(self):\n",
    "        self.model.load_weights(self.weights)\n",
    "\n",
    "        \n",
    "class frac_pool_net(max_pool_net):\n",
    "    def __init__(self, opt, batch_input_shape, classes, k_size=(3,3), filters=2, blocks=2):\n",
    "        ratio = 1.35 # 1.33 blocks=3\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.InputLayer(batch_input_shape=batch_input_shape))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same'))\n",
    "        self.model.add(FractionalPooling2D(pool_ratio=(1, ratio, ratio, 1),pseudo_random = True, overlap=False))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same'))\n",
    "        self.model.add(FractionalPooling2D(pool_ratio=(1, ratio, ratio, 1),pseudo_random = True, overlap=False))\n",
    "        for i in range(2, blocks+1):\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same'))\n",
    "            self.model.add(FractionalPooling2D(pool_ratio=(1, ratio, ratio, 1),pseudo_random = True, overlap=False))\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same'))\n",
    "            self.model.add(FractionalPooling2D(pool_ratio=(1, ratio, ratio, 1),pseudo_random = True, overlap=False))\n",
    "        self.model.add(keras.layers.Flatten())\n",
    "        self.model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "        self.model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    \n",
    "class ft_pool_net(max_pool_net):\n",
    "    def __init__(self, opt, batch_input_shape, classes, k_size=(3,3), filters=2, blocks=2):\n",
    "        strides=(math.sqrt(2), math.sqrt(2))\n",
    "        pool_size=(math.sqrt(2), math.sqrt(2))\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.InputLayer(batch_input_shape=batch_input_shape))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same',))\n",
    "        self.model.add(keras.layers.Lambda(lambda x: ft_pool(x, strides, pool_size)))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same'))\n",
    "        self.model.add(keras.layers.Lambda(lambda x: ft_pool(x, strides, pool_size)))\n",
    "        for i in range(2, blocks+1):\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same'))\n",
    "            self.model.add(keras.layers.Lambda(lambda x: ft_pool(x, strides, pool_size)))\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same'))\n",
    "            self.model.add(keras.layers.Lambda(lambda x: ft_pool(x, strides, pool_size)))\n",
    "        self.model.add(keras.layers.Flatten())\n",
    "        self.model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "        self.model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "        \n",
    "class avg_pool_net(max_pool_net):\n",
    "    def __init__(self, opt, batch_input_shape, classes, k_size=(3,3), filters=2, blocks=2):\n",
    "        pool_size=2\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.InputLayer(batch_input_shape=batch_input_shape))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same',))\n",
    "        self.model.add(keras.layers.AveragePooling2D(pool_size=(pool_size, pool_size), strides=(2,2), padding='same'))\n",
    "        self.model.add(keras.layers.Conv2D(filters, k_size, activation='relu', padding='same'))\n",
    "        for i in range(2, blocks+1):\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same'))\n",
    "            self.model.add(keras.layers.AveragePooling2D(pool_size=(pool_size, pool_size), strides=(2,2), padding='same'))\n",
    "            self.model.add(keras.layers.Conv2D(filters*i, k_size, activation='relu', padding='same'))\n",
    "        self.model.add(keras.layers.Flatten())\n",
    "        self.model.add(keras.layers.Dense(classes, activation='softmax'))\n",
    "        self.model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        print(self.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 100\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (100, 32, 32, 2)          56        \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (100, 23, 23, 2)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (100, 23, 23, 2)          38        \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (100, 16, 16, 2)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (100, 16, 16, 4)          76        \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (100, 11, 11, 4)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (100, 11, 11, 4)          148       \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (100, 8, 8, 4)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (100, 256)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 10)                 2570      \n",
      "=================================================================\n",
      "Total params: 2,888\n",
      "Trainable params: 2,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "training on array data, network type: ft_pool_net\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1_13/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 2.1471 - accuracy: 0.1893 - val_loss: 1.9443 - val_accuracy: 0.2861\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 1.9115 - accuracy: 0.2954 - val_loss: 1.8635 - val_accuracy: 0.3241\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 1.8383 - accuracy: 0.3303 - val_loss: 1.8002 - val_accuracy: 0.3502\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 1.7676 - accuracy: 0.3655 - val_loss: 1.7330 - val_accuracy: 0.3852\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 1.6931 - accuracy: 0.4000 - val_loss: 1.6647 - val_accuracy: 0.4160\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 1.6485 - accuracy: 0.4175 - val_loss: 1.6350 - val_accuracy: 0.4273\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 23s 458us/step - loss: 1.6174 - accuracy: 0.4297 - val_loss: 1.6113 - val_accuracy: 0.4338\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 1.5906 - accuracy: 0.4409 - val_loss: 1.5822 - val_accuracy: 0.4424\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 1.5679 - accuracy: 0.4484 - val_loss: 1.5615 - val_accuracy: 0.4468\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 1.5476 - accuracy: 0.4568 - val_loss: 1.5451 - val_accuracy: 0.4542\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 1.5314 - accuracy: 0.4628 - val_loss: 1.5467 - val_accuracy: 0.4534\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 1.5146 - accuracy: 0.4676 - val_loss: 1.5221 - val_accuracy: 0.4653\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 1.5042 - accuracy: 0.4713 - val_loss: 1.5025 - val_accuracy: 0.4684\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 1.4921 - accuracy: 0.4753 - val_loss: 1.5078 - val_accuracy: 0.4665\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 23s 452us/step - loss: 1.4810 - accuracy: 0.4785 - val_loss: 1.5031 - val_accuracy: 0.4682\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 1.4713 - accuracy: 0.4829 - val_loss: 1.4807 - val_accuracy: 0.4822\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 1.4617 - accuracy: 0.4856 - val_loss: 1.4820 - val_accuracy: 0.4789\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 1.4529 - accuracy: 0.4888 - val_loss: 1.4794 - val_accuracy: 0.4767\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 21s 422us/step - loss: 1.4470 - accuracy: 0.4901 - val_loss: 1.4683 - val_accuracy: 0.4811\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 16s 325us/step - loss: 1.4396 - accuracy: 0.4939 - val_loss: 1.4598 - val_accuracy: 0.4879\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 16s 326us/step - loss: 1.4347 - accuracy: 0.4954 - val_loss: 1.4563 - val_accuracy: 0.4919\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 16s 322us/step - loss: 1.4295 - accuracy: 0.4980 - val_loss: 1.4479 - val_accuracy: 0.4925\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 20s 401us/step - loss: 1.4244 - accuracy: 0.4993 - val_loss: 1.4366 - val_accuracy: 0.4979\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 19s 374us/step - loss: 1.4180 - accuracy: 0.5027 - val_loss: 1.4602 - val_accuracy: 0.4863\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 16s 310us/step - loss: 1.4150 - accuracy: 0.5033 - val_loss: 1.4443 - val_accuracy: 0.4938\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 1.4107 - accuracy: 0.5030 - val_loss: 1.4315 - val_accuracy: 0.4988\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 1.4076 - accuracy: 0.5033 - val_loss: 1.4255 - val_accuracy: 0.4988\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 1.4031 - accuracy: 0.5037 - val_loss: 1.4313 - val_accuracy: 0.4951\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 15s 299us/step - loss: 1.4015 - accuracy: 0.5081 - val_loss: 1.4281 - val_accuracy: 0.4977\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 1.3963 - accuracy: 0.5089 - val_loss: 1.4197 - val_accuracy: 0.5020\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 1.3952 - accuracy: 0.5072 - val_loss: 1.4135 - val_accuracy: 0.5045\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 15s 301us/step - loss: 1.3922 - accuracy: 0.5089 - val_loss: 1.4186 - val_accuracy: 0.4984\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 15s 299us/step - loss: 1.3882 - accuracy: 0.5099 - val_loss: 1.4166 - val_accuracy: 0.5005\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 15s 299us/step - loss: 1.3871 - accuracy: 0.5118 - val_loss: 1.4266 - val_accuracy: 0.4956\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 1.3837 - accuracy: 0.5124 - val_loss: 1.4176 - val_accuracy: 0.5019\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 15s 300us/step - loss: 1.3821 - accuracy: 0.5144 - val_loss: 1.4422 - val_accuracy: 0.4924\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.5.\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 15s 299us/step - loss: 1.3661 - accuracy: 0.5194 - val_loss: 1.4019 - val_accuracy: 0.5067\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 1.3645 - accuracy: 0.5203 - val_loss: 1.4075 - val_accuracy: 0.5061\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 16s 318us/step - loss: 1.3628 - accuracy: 0.5203 - val_loss: 1.4011 - val_accuracy: 0.5068\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 1.3623 - accuracy: 0.5205 - val_loss: 1.4059 - val_accuracy: 0.5066\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 16s 320us/step - loss: 1.3609 - accuracy: 0.5216 - val_loss: 1.3990 - val_accuracy: 0.5103\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 16s 321us/step - loss: 1.3601 - accuracy: 0.5220 - val_loss: 1.4011 - val_accuracy: 0.5082\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 16s 322us/step - loss: 1.3589 - accuracy: 0.5220 - val_loss: 1.3974 - val_accuracy: 0.5068\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 16s 316us/step - loss: 1.3577 - accuracy: 0.5220 - val_loss: 1.3950 - val_accuracy: 0.5100\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 1.3566 - accuracy: 0.5224 - val_loss: 1.3935 - val_accuracy: 0.5119\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 16s 323us/step - loss: 1.3571 - accuracy: 0.5228 - val_loss: 1.4240 - val_accuracy: 0.5004\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 16s 329us/step - loss: 1.3553 - accuracy: 0.5234 - val_loss: 1.4020 - val_accuracy: 0.5067\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 1.3550 - accuracy: 0.5242 - val_loss: 1.3942 - val_accuracy: 0.5063\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 17s 339us/step - loss: 1.3541 - accuracy: 0.5236 - val_loss: 1.3939 - val_accuracy: 0.5072\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 17s 340us/step - loss: 1.3537 - accuracy: 0.5243 - val_loss: 1.3950 - val_accuracy: 0.5092\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.25.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (100, 32, 32, 2)          56        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (100, 16, 16, 2)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (100, 16, 16, 2)          38        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (100, 16, 16, 4)          76        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (100, 8, 8, 4)            0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (100, 8, 8, 4)            148       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (100, 256)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 10)                 2570      \n",
      "=================================================================\n",
      "Total params: 2,888\n",
      "Trainable params: 2,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "training on array data, network type: avg_pool_net\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 9s 181us/step - loss: 1.9930 - accuracy: 0.2743 - val_loss: 1.7842 - val_accuracy: 0.3632\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 9s 180us/step - loss: 1.7274 - accuracy: 0.3795 - val_loss: 1.6362 - val_accuracy: 0.4124\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 9s 181us/step - loss: 1.6283 - accuracy: 0.4137 - val_loss: 1.5845 - val_accuracy: 0.4330\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 9s 178us/step - loss: 1.5871 - accuracy: 0.4283 - val_loss: 1.5497 - val_accuracy: 0.4390\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 9s 179us/step - loss: 1.5566 - accuracy: 0.4374 - val_loss: 1.5430 - val_accuracy: 0.4453\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 9s 180us/step - loss: 1.5346 - accuracy: 0.4468 - val_loss: 1.5128 - val_accuracy: 0.4522\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 9s 179us/step - loss: 1.5166 - accuracy: 0.4549 - val_loss: 1.5011 - val_accuracy: 0.4526\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 9s 187us/step - loss: 1.5017 - accuracy: 0.4618 - val_loss: 1.4874 - val_accuracy: 0.4678\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 9s 188us/step - loss: 1.4924 - accuracy: 0.4649 - val_loss: 1.5144 - val_accuracy: 0.4542\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 9s 188us/step - loss: 1.4792 - accuracy: 0.4688 - val_loss: 1.4654 - val_accuracy: 0.4688\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 1.4720 - accuracy: 0.4710 - val_loss: 1.4524 - val_accuracy: 0.4768\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 1.4622 - accuracy: 0.4759 - val_loss: 1.4536 - val_accuracy: 0.4775\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 1.4558 - accuracy: 0.4793 - val_loss: 1.4401 - val_accuracy: 0.4810\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 9s 183us/step - loss: 1.4501 - accuracy: 0.4804 - val_loss: 1.4349 - val_accuracy: 0.4803\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 9s 178us/step - loss: 1.4441 - accuracy: 0.4822 - val_loss: 1.4354 - val_accuracy: 0.4834\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 1.4380 - accuracy: 0.4875 - val_loss: 1.4264 - val_accuracy: 0.4879\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 9s 180us/step - loss: 1.4330 - accuracy: 0.4880 - val_loss: 1.4406 - val_accuracy: 0.4843\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 1.4276 - accuracy: 0.4904 - val_loss: 1.4393 - val_accuracy: 0.4823\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 9s 185us/step - loss: 1.4227 - accuracy: 0.4926 - val_loss: 1.4431 - val_accuracy: 0.4830\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 1.4192 - accuracy: 0.4923 - val_loss: 1.4371 - val_accuracy: 0.4864\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 9s 178us/step - loss: 1.4170 - accuracy: 0.4926 - val_loss: 1.4213 - val_accuracy: 0.4916\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 1.4117 - accuracy: 0.4963 - val_loss: 1.3984 - val_accuracy: 0.4972\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 1.4065 - accuracy: 0.4990 - val_loss: 1.4113 - val_accuracy: 0.4964\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 9s 184us/step - loss: 1.4021 - accuracy: 0.5018 - val_loss: 1.3937 - val_accuracy: 0.4985\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 9s 186us/step - loss: 1.4001 - accuracy: 0.4997 - val_loss: 1.4029 - val_accuracy: 0.4971\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 9s 181us/step - loss: 1.3969 - accuracy: 0.5033 - val_loss: 1.3855 - val_accuracy: 0.5043\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 1.3933 - accuracy: 0.5042 - val_loss: 1.3870 - val_accuracy: 0.5012\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 9s 183us/step - loss: 1.3894 - accuracy: 0.5049 - val_loss: 1.3872 - val_accuracy: 0.5009\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 1.3872 - accuracy: 0.5066 - val_loss: 1.3847 - val_accuracy: 0.5052\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 9s 183us/step - loss: 1.3857 - accuracy: 0.5047 - val_loss: 1.3799 - val_accuracy: 0.5043\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 1.3838 - accuracy: 0.5062 - val_loss: 1.3812 - val_accuracy: 0.5051\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 1.3801 - accuracy: 0.5089 - val_loss: 1.3688 - val_accuracy: 0.5071\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 9s 175us/step - loss: 1.3769 - accuracy: 0.5094 - val_loss: 1.3828 - val_accuracy: 0.5034\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 9s 180us/step - loss: 1.3759 - accuracy: 0.5104 - val_loss: 1.3743 - val_accuracy: 0.5061\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 9s 176us/step - loss: 1.3723 - accuracy: 0.5123 - val_loss: 1.3777 - val_accuracy: 0.5014\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 1.3710 - accuracy: 0.5130 - val_loss: 1.3668 - val_accuracy: 0.5112\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 9s 171us/step - loss: 1.3680 - accuracy: 0.5140 - val_loss: 1.3628 - val_accuracy: 0.5132\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 9s 172us/step - loss: 1.3654 - accuracy: 0.5132 - val_loss: 1.3661 - val_accuracy: 0.5091\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 1.3637 - accuracy: 0.5158 - val_loss: 1.3593 - val_accuracy: 0.5120\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 1.3609 - accuracy: 0.5147 - val_loss: 1.3795 - val_accuracy: 0.5061\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 1.3593 - accuracy: 0.5180 - val_loss: 1.3687 - val_accuracy: 0.5101\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 9s 189us/step - loss: 1.3565 - accuracy: 0.5185 - val_loss: 1.3830 - val_accuracy: 0.5013\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 10s 192us/step - loss: 1.3553 - accuracy: 0.5184 - val_loss: 1.4302 - val_accuracy: 0.4882\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 1.3544 - accuracy: 0.5178 - val_loss: 1.3580 - val_accuracy: 0.5126\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 1.3517 - accuracy: 0.5208 - val_loss: 1.3470 - val_accuracy: 0.5116\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 9s 174us/step - loss: 1.3516 - accuracy: 0.5221 - val_loss: 1.3732 - val_accuracy: 0.5103\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 9s 175us/step - loss: 1.3502 - accuracy: 0.5223 - val_loss: 1.3581 - val_accuracy: 0.5140\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 1.3474 - accuracy: 0.5204 - val_loss: 1.3491 - val_accuracy: 0.5110\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 9s 173us/step - loss: 1.3473 - accuracy: 0.5208 - val_loss: 1.3474 - val_accuracy: 0.5165\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 9s 177us/step - loss: 1.3431 - accuracy: 0.5253 - val_loss: 1.3410 - val_accuracy: 0.5160\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (100, 32, 32, 2)          56        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (100, 32, 32, 2)          38        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (100, 16, 16, 2)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (100, 16, 16, 4)          76        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (100, 16, 16, 4)          148       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (100, 8, 8, 4)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (100, 256)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (100, 10)                 2570      \n",
      "=================================================================\n",
      "Total params: 2,888\n",
      "Trainable params: 2,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "training on array data, network type: max_pool_net\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 14s 275us/step - loss: 1.9436 - accuracy: 0.2928 - val_loss: 1.7189 - val_accuracy: 0.3827\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 14s 275us/step - loss: 1.6571 - accuracy: 0.4042 - val_loss: 1.6432 - val_accuracy: 0.4173\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 14s 276us/step - loss: 1.5614 - accuracy: 0.4422 - val_loss: 1.5312 - val_accuracy: 0.4456\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 14s 283us/step - loss: 1.5214 - accuracy: 0.4566 - val_loss: 1.5262 - val_accuracy: 0.4555\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 15s 309us/step - loss: 1.4965 - accuracy: 0.4662 - val_loss: 1.4829 - val_accuracy: 0.4667\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 1.4809 - accuracy: 0.4727 - val_loss: 1.4882 - val_accuracy: 0.4714\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 14s 275us/step - loss: 1.4674 - accuracy: 0.4780 - val_loss: 1.4666 - val_accuracy: 0.4766\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 14s 274us/step - loss: 1.4583 - accuracy: 0.4822 - val_loss: 1.4628 - val_accuracy: 0.4762\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 14s 274us/step - loss: 1.4464 - accuracy: 0.4872 - val_loss: 1.4511 - val_accuracy: 0.4813\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 15s 291us/step - loss: 1.4363 - accuracy: 0.4917 - val_loss: 1.4684 - val_accuracy: 0.4807\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 14s 279us/step - loss: 1.4285 - accuracy: 0.4961 - val_loss: 1.4286 - val_accuracy: 0.4947\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 14s 279us/step - loss: 1.4205 - accuracy: 0.4990 - val_loss: 1.4267 - val_accuracy: 0.4926\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 14s 277us/step - loss: 1.4119 - accuracy: 0.4991 - val_loss: 1.4463 - val_accuracy: 0.4862\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 14s 273us/step - loss: 1.4056 - accuracy: 0.5032 - val_loss: 1.4186 - val_accuracy: 0.4967\n",
      "Epoch 15/50\n",
      "45300/50000 [==========================>...] - ETA: 1s - loss: 1.3973 - accuracy: 0.5079"
     ]
    }
   ],
   "source": [
    "ft_net = ft_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "ft_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "ft_net.restart_session()\n",
    "\n",
    "avg_net = avg_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "avg_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "avg_net.restart_session()\n",
    "\n",
    "max_net = max_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "max_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "max_net.restart_session()\n",
    "\n",
    "frac_net = frac_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "frac_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "frac_net.restart_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline best 2x max pool = 53.19%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(max_net.get_history().history['val_accuracy'], 'r')\n",
    "plt.plot(frac_net.get_history().history['val_accuracy'], 'g')\n",
    "plt.plot(ft_net.get_history().history['val_accuracy'], 'b')\n",
    "plt.plot(avg_net.get_history().history['val_accuracy'], 'c')\n",
    "plt.show()\n",
    "# petruv pristup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(max_net.get_history().history['val_accuracy'], 'r')\n",
    "plt.plot(frac_net.get_history().history['val_accuracy'], 'g')\n",
    "plt.plot(ft_net.get_history().history['val_accuracy'], 'b')\n",
    "plt.plot(avg_net.get_history().history['val_accuracy'], 'c')\n",
    "plt.show()\n",
    "# muj pristup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(max_net.get_history().history['val_accuracy'], 'r')\n",
    "plt.plot(frac_net.get_history().history['val_accuracy'], 'g')\n",
    "plt.plot(ft_net.get_history().history['val_accuracy'], 'b')\n",
    "plt.plot(avg_net.get_history().history['val_accuracy'], 'c')\n",
    "plt.show()\n",
    "# 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ft_net = ft_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "ft_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "ft_net.restart_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(max_net.get_history().history['val_accuracy'], 'r')\n",
    "plt.plot(frac_net.get_history().history['val_accuracy'], 'g')\n",
    "plt.plot(ft_net.get_history().history['val_accuracy'], 'b')\n",
    "plt.plot(avg_net.get_history().history['val_accuracy'], 'c')\n",
    "plt.show()\n",
    "# 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_net = avg_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "avg_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "avg_net.restart_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(max_net.get_history().history['val_accuracy'], 'r')\n",
    "plt.plot(frac_net.get_history().history['val_accuracy'], 'g')\n",
    "plt.plot(ft_net.get_history().history['val_accuracy'], 'b')\n",
    "plt.plot(avg_net.get_history().history['val_accuracy'], 'c')\n",
    "plt.show()\n",
    "# 3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_net = avg_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "avg_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "avg_net.restart_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(max_net.get_history().history['val_accuracy'], 'r')\n",
    "plt.plot(frac_net.get_history().history['val_accuracy'], 'g')\n",
    "plt.plot(ft_net.get_history().history['val_accuracy'], 'b')\n",
    "plt.plot(avg_net.get_history().history['val_accuracy'], 'c')\n",
    "plt.show()\n",
    "# 4x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_net = avg_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "avg_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "avg_net.restart_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(max_net.get_history().history['val_accuracy'], 'r')\n",
    "plt.plot(frac_net.get_history().history['val_accuracy'], 'g')\n",
    "plt.plot(ft_net.get_history().history['val_accuracy'], 'b')\n",
    "plt.plot(avg_net.get_history().history['val_accuracy'], 'c')\n",
    "plt.show()\n",
    "# 6x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_net = avg_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "avg_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "avg_net.restart_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(max_net.get_history().history['val_accuracy'], 'r')\n",
    "plt.plot(frac_net.get_history().history['val_accuracy'], 'g')\n",
    "plt.plot(ft_net.get_history().history['val_accuracy'], 'b')\n",
    "plt.plot(avg_net.get_history().history['val_accuracy'], 'c')\n",
    "plt.show()\n",
    "# 7x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_net = ft_pool_net(keras.optimizers.Adadelta(), (BATCH_SIZE,) + x_train.shape[1:], y_train.shape[1], blocks=2)\n",
    "ft_net.train(BATCH_SIZE, EPOCHS, train_data=(x_train, y_train, x_test, y_test), callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=.5, verbose=1)])\n",
    "ft_net.restart_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(max_net.get_history().history['val_accuracy'], 'r')\n",
    "plt.plot(frac_net.get_history().history['val_accuracy'], 'g')\n",
    "plt.plot(ft_net.get_history().history['val_accuracy'], 'b')\n",
    "plt.plot(avg_net.get_history().history['val_accuracy'], 'c')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
